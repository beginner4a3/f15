import io
import spaces
import torch
import librosa
import requests
import tempfile
import numpy as np
import gradio as gr
import soundfile as sf
from transformers import AutoModel

# ------------------------------------------------------------
#  1Ô∏è‚É£  Flash‚ÄëAttention / SDPA & TF32 settings (run once)
# ------------------------------------------------------------
if torch.cuda.is_available():
    # Flash‚ÄëAttention / SDPA
    try:
        torch.backends.cuda.enable_flash_sdp(True)
        torch.backends.cuda.enable_mem_efficient_sdp(True)
        print("‚úÖ Flash Attention / SDPA enabled")
    except Exception as e:
        print(f"‚ö†Ô∏è Flash Attention not available: {e}")

    # TF32 for faster matrix ops on Ampere+ GPUs
    torch.backends.cuda.matmul.allow_tf32 = True
    torch.backends.cudnn.allow_tf32 = True
    torch.backends.cudnn.benchmark = True
    print("‚úÖ CUDA optimizations enabled (TF32, cuDNN benchmark)")

# ------------------------------------------------------------
#  2Ô∏è‚É£  Helper: download reference audio
# ------------------------------------------------------------
def load_audio_from_url(url: str):
    resp = requests.get(url)
    if resp.status_code == 200:
        audio_data, sr = sf.read(io.BytesIO(resp.content))
        return sr, audio_data
    return None, None

# ------------------------------------------------------------
#  3Ô∏è‚É£  Model loading ‚Äì FP16 + compile
# ------------------------------------------------------------
repo_id = "ai4bharat/IndicF5"
print("Loading model:", repo_id)

# Load directly in half‚Äëprecision (FP16)
model = AutoModel.from_pretrained(
    repo_id,
    trust_remote_code=True,
    low_cpu_mem_usage=False,          # must stay False ‚Äì prevents meta tensors
    torch_dtype=torch.float16,         # FP16 for speed
    token=os.getenv("HF_TOKEN") if os.getenv("HF_TOKEN") else None,
)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Device:", device)
model = model.to(device)

# Optional torch.compile ‚Äì huge inference speed boost
if hasattr(torch, "compile"):
    try:
        model = torch.compile(model, mode="reduce-overhead")
        print("‚úÖ torch.compile enabled (reduce-overhead)")
    except Exception as e:
        print(f"‚ö†Ô∏è torch.compile failed: {e}")

# ------------------------------------------------------------
#  4Ô∏è‚É£  Inference wrapper ‚Äì inference mode + autocast
# ------------------------------------------------------------
@gpu_decorator
def synthesize_speech(text, ref_audio, ref_text):
    # Basic validation
    if ref_audio is None or not ref_text.strip():
        return "Error: Please provide a reference audio and its corresponding text."

    # Unpack reference audio (Gradio gives (sr, np.ndarray))
    if isinstance(ref_audio, tuple) and len(ref_audio) == 2:
        sample_rate, audio_data = ref_audio
    else:
        return "Error: Invalid reference audio input."

    # Write temporary wav (no resampling needed)
    with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp_wav:
        sf.write(tmp_wav.name, audio_data, samplerate=sample_rate, format="WAV")
        tmp_wav.flush()

    # Fast inference
    with torch.inference_mode(), torch.cuda.amp.autocast(enabled=True):
        # The IndicF5 model implements __call__ as the inference entry point
        audio = model(text, ref_audio_path=tmp_wav.name, ref_text=ref_text)

    # Normalise int16 ‚Üí float32 if the vocoder returned PCM16
    if audio.dtype == np.int16:
        audio = audio.astype(np.float32) / 32768.0

    return 24000, audio

# ------------------------------------------------------------
#  5Ô∏è‚É£  Example data (unchanged, just kept for UI)
# ------------------------------------------------------------
EXAMPLES = [
    {
        "audio_name": "PAN_F (Happy)",
        "audio_url": "https://github.com/AI4Bharat/IndicF5/raw/refs/heads/main/prompts/PAN_F_HAPPY_00002.wav",
        "ref_text": "‡®á‡©±‡®ï ‡®ó‡©ç‡®∞‡®æ‡®π‡®ï ‡®®‡©á ‡®∏‡®æ‡®°‡©Ä ‡®¨‡©á‡®Æ‡®ø‡®∏‡®æ‡®≤ ‡®∏‡©á‡®µ‡®æ ‡®¨‡®æ‡®∞‡©á ‡®¶‡®ø‡®≤‡©ã‡®Ç‡®ó‡®µ‡®æ‡®π‡©Ä ‡®¶‡®ø‡©±‡®§‡©Ä ‡®ú‡®ø‡®∏ ‡®®‡®æ‡®≤ ‡®∏‡®æ‡®®‡©Ç‡©∞ ‡®Ö‡®®‡©∞‡®¶ ‡®Æ‡®π‡®ø‡®∏‡©Ç‡®∏ ‡®π‡©ã‡®á‡®Ü‡•§",
        "synth_text": "‡§Æ‡•à‡§Ç ‡§¨‡§ø‡§®‡§æ ‡§ï‡§ø‡§∏‡•Ä ‡§ö‡§ø‡§Ç‡§§‡§æ ‡§ï‡•á ‡§Ö‡§™‡§®‡•á ‡§¶‡•ã‡§∏‡•ç‡§§‡•ã‡§Ç ‡§ï‡•ã ‡§Ö‡§™‡§®‡•á ‡§ë‡§ü‡•ã‡§Æ‡•ã‡§¨‡§æ‡§á‡§≤ ‡§è‡§ï‡•ç‡§∏‡§™‡§∞‡•ç‡§ü ‡§ï‡•á ‡§™‡§æ‡§∏ ‡§≠‡•á‡§ú ‡§¶‡•á‡§§‡§æ ‡§π‡•Ç‡§Å ‡§ï‡•ç‡§Ø‡•ã‡§Ç‡§ï‡§ø ‡§Æ‡•à‡§Ç ‡§ú‡§æ‡§®‡§§‡§æ ‡§π‡•Ç‡§Å ‡§ï‡§ø ‡§µ‡§π ‡§®‡§ø‡§∂‡•ç‡§ö‡§ø‡§§ ‡§∞‡•Ç‡§™ ‡§∏‡•á ‡§â‡§®‡§ï‡•Ä ‡§∏‡§≠‡•Ä ‡§ú‡§∞‡•Ç‡§∞‡§§‡•ã‡§Ç ‡§™‡§∞ ‡§ñ‡§∞‡§æ ‡§â‡§§‡§∞‡•á‡§ó‡§æ‡•§"
    },
    {
        "audio_name": "TAM_F (Happy)",
        "audio_url": "https://github.com/AI4Bharat/IndicF5/raw/refs/heads/main/prompts/TAM_F_HAPPY_00001.wav",
        "ref_text": "‡Æ®‡Ææ‡Æ©‡Øç ‡Æ®‡ØÜ‡Æ©‡Æö‡Øç‡Æö ‡ÆÆ‡Ææ‡Æ§‡Æø‡Æ∞‡Æø‡ÆØ‡Øá ‡ÆÖ‡ÆÆ‡Øá‡Æö‡Ææ‡Æ©‡Øç‡Æ≤ ‡Æ™‡ØÜ‡Æ∞‡Æø‡ÆØ ‡Æ§‡Æ≥‡Øç‡Æ≥‡ØÅ‡Æ™‡Æü‡Æø ‡Æµ‡Æ®‡Øç‡Æ§‡Æø‡Æ∞‡ØÅ‡Æï‡Øç‡Æï‡ØÅ. ‡Æï‡ÆÆ‡Øç‡ÆÆ‡Æø ‡Æï‡Ææ‡Æö‡ØÅ‡Æï‡Øç‡Æï‡Øá ‡ÆÖ‡Æ®‡Øç‡Æ§‡Æ™‡Øç ‡Æ™‡ØÅ‡Æ§‡ØÅ ‡Æö‡Øá‡ÆÆ‡Øç‡Æö‡Æô‡Øç ‡ÆÆ‡Ææ‡Æü‡Æ≤ ‡Æµ‡Ææ‡Æô‡Øç‡Æï‡Æø‡Æü‡Æ≤‡Ææ‡ÆÆ‡Øç.",
        "synth_text": "‡¥≠‡¥ï‡µç‡¥∑‡¥£‡¥§‡µç‡¥§‡¥ø‡¥®‡µç ‡¥∂‡µá‡¥∑‡¥Ç ‡¥§‡µà‡¥∞‡µç ‡¥∏‡¥æ‡¥¶‡¥Ç ‡¥ï‡¥¥‡¥ø‡¥ö‡µç‡¥ö‡¥æ‡µΩ ‡¥í‡¥∞‡µÅ ‡¥â‡¥∑‡¥æ‡¥±‡¥æ‡¥£‡µç!"
    },
    {
        "audio_name": "MAR_F (WIKI)",
        "audio_url": "https://github.com/AI4Bharat/IndicF5/raw/refs/heads/main/prompts/MAR_F_WIKI_00001.wav",
        "ref_text": "‡§¶‡§ø‡§ó‡§Ç‡§ü‡§∞‡§æ‡§µ‡•ç‡§¶‡§æ‡§∞‡•á ‡§Ö‡§Ç‡§§‡§∞‡§æ‡§≥ ‡§ï‡§ï‡•ç‡§∑‡•á‡§§‡§≤‡§æ ‡§ï‡§ö‡§∞‡§æ ‡§ö‡§ø‡§®‡•ç‡§π‡§ø‡§§ ‡§ï‡§∞‡§£‡•ç‡§Ø‡§æ‡§∏‡§æ‡§†‡•Ä ‡§™‡•ç‡§∞‡§Ø‡§§‡•ç‡§® ‡§ï‡•á‡§≤‡•á ‡§ú‡§æ‡§§ ‡§Ü‡§π‡•á.",
        "synth_text": "‡§™‡•ç‡§∞‡§æ‡§∞‡§Ç‡§≠‡§ø‡§ï ‡§Ö‡§Ç‡§ï‡•Å‡§∞ ‡§õ‡•á‡§¶‡§ï. ‡§Æ‡•Ä ‡§∏‡•ã‡§≤‡§æ‡§™‡•Ç‡§∞ ‡§ú‡§ø‡§≤‡•ç‡§π‡•ç‡§Ø‡§æ‡§§‡•Ä‡§≤ ‡§Æ‡§æ‡§≥‡§∂‡§ø‡§∞‡§∏ ‡§§‡§æ‡§≤‡•Å‡§ï‡•ç‡§Ø‡§æ‡§§‡•Ä‡§≤ ‡§∂‡•á‡§§‡§ï‡§∞‡•Ä ‡§ó‡§£‡§™‡§§ ‡§™‡§æ‡§ü‡•Ä‡§≤ ‡§¨‡•ã‡§≤‡§§‡•ã‡§Ø. ‡§Æ‡§æ‡§ù‡•ç‡§Ø‡§æ ‡§ä‡§∏ ‡§™‡§ø‡§ï‡§æ‡§µ‡§∞ ‡§™‡•ç‡§∞‡§æ‡§∞‡§Ç‡§≠‡§ø‡§ï ‡§Ö‡§Ç‡§ï‡•Å‡§∞ ‡§õ‡•á‡§¶‡§ï ‡§ï‡•Ä‡§° ‡§Ü‡§¢‡§≥‡§§ ‡§Ü‡§π‡•á. ‡§ï‡•ç‡§≤‡•ã‡§∞‡§Å‡§ü‡•ç‡§∞‡§æ‡§®‡§ø‡§≤‡•Ä‡§™‡•ç‡§∞‡•ã‡§≤ (‡§ï‡•ã‡§∞‡§æ‡§ú‡•á‡§®) ‡§µ‡§æ‡§™‡§∞‡§£‡•á ‡§Ø‡•ã‡§ó‡•ç‡§Ø ‡§Ü‡§π‡•á ‡§ï‡§æ? ‡§§‡•ç‡§Ø‡§æ‡§ö‡•á ‡§™‡•ç‡§∞‡§Æ‡§æ‡§£ ‡§ï‡§ø‡§§‡•Ä ‡§Ö‡§∏‡§æ‡§µ‡•á?"
    },
    {
        "audio_name": "MAR_M (WIKI)",
        "audio_url": "https://github.com/AI4Bharat/IndicF5/raw/refs/heads/main/prompts/MAR_M_WIKI_00001.wav",
        "ref_text": "‡§Ø‡§æ ‡§™‡•ç‡§∞‡§•‡§æ‡§≤‡§æ ‡§è‡§ï‡•ã‡§£‡•Ä‡§∏‡§∂‡•á ‡§™‡§Ç‡§ö‡§æ‡§§‡§∞ ‡§à‡§∏‡§µ‡•Ä ‡§™‡§æ‡§∏‡•Ç‡§® ‡§≠‡§æ‡§∞‡§§‡•Ä‡§Ø ‡§¶‡§Ç‡§° ‡§∏‡§Ç‡§π‡§ø‡§§‡§æ‡§ö‡•Ä ‡§ß‡§æ‡§∞‡§æ ‡§ö‡§æ‡§∞‡§∂‡•á ‡§Ö‡§†‡•ç‡§†‡§æ‡§µ‡•Ä‡§∏ ‡§Ü‡§£‡§ø ‡§ö‡§æ‡§∞‡§∂‡•á ‡§è‡§ï‡•ã‡§£‡§§‡•Ä‡§∏‡§ö‡•ç‡§Ø‡§æ ‡§Ö‡§®‡•ç‡§§‡§∞‡•ç‡§ó‡§§ ‡§®‡§ø‡§∑‡•á‡§ß ‡§ï‡•á‡§≤‡§æ.",
        "synth_text": "‡§ú‡•Ä‡§µ‡§æ‡§£‡•Ç ‡§ï‡§∞‡§™‡§æ. ‡§Æ‡•Ä ‡§Ö‡§π‡§Æ‡§¶‡§®‡§ó‡§∞ ‡§ú‡§ø‡§≤‡•ç‡§π‡•ç‡§Ø‡§æ‡§§‡•Ä‡§≤ ‡§∞‡§æ‡§π‡•Å‡§∞‡•Ä ‡§ó‡§æ‡§µ‡§æ‡§§‡•Ç‡§® ‡§¨‡§æ‡§≥‡§æ‡§∏‡§æ‡§π‡•á‡§¨ ‡§ú‡§æ‡§ß‡§µ ‡§¨‡•ã‡§≤‡§§‡•ã‡§Ø. ‡§Æ‡§æ‡§ù‡•ç‡§Ø‡§æ ‡§°‡§æ‡§≥‡§ø‡§Ç‡§¨ ‡§¨‡§æ‡§ó‡•á‡§§ ‡§ú‡•Ä‡§µ‡§æ‡§£‡•Ç ‡§ï‡§∞‡§™‡§æ ‡§Æ‡•ã‡§†‡•ç‡§Ø‡§æ ‡§™‡•ç‡§∞‡§Æ‡§æ‡§£‡§æ‡§§ ‡§¶‡§ø‡§∏‡§§‡•ã‡§Ø. ‡§∏‡•ç‡§ü‡•ç‡§∞‡•á‡§™‡•ç‡§ü‡•ã‡§∏‡§æ‡§Ø‡§ï‡•ç‡§≤‡§ø‡§® ‡§Ü‡§£‡§ø ‡§ï‡•â‡§™‡§∞ ‡§ë‡§ï‡•ç‡§∏‡§ø‡§°‡•ç‡§≤‡•ã‡§∞‡§æ‡§à‡§° ‡§´‡§µ‡§æ‡§∞‡§£‡•Ä‡§∏‡§æ‡§†‡•Ä ‡§Ø‡•ã‡§ó‡•ç‡§Ø ‡§™‡•ç‡§∞‡§Æ‡§æ‡§£ ‡§ï‡§æ‡§Ø ‡§Ö‡§∏‡§æ‡§µ‡•á?"
    },
    {
        "audio_name": "KAN_F (Happy)",
        "audio_url": "https://github.com/AI4Bharat/IndicF5/raw/refs/heads/main/prompts/KAN_F_HAPPY_00001.wav",
        "ref_text": "‡≤®‡≤Æ‡≥ç‚Äå ‡≤´‡≥ç‡≤∞‡≤ø‡≤ú‡≥ç‡≤ú‡≤≤‡≥ç‡≤≤‡≤ø  ‡≤ï‡≥Ç‡≤≤‡≤ø‡≤Ç‡≤ó‡≥ç‚Äå ‡≤∏‡≤Æ‡≤∏‡≥ç‡≤Ø‡≥Ü ‡≤Ü‡≤ó‡≤ø ‡≤®‡≤æ‡≤®‡≥ç‚Äå ‡≤≠‡≤æ‡≤≥ ‡≤¶‡≤ø‡≤®‡≤¶‡≤ø‡≤Ç‡≤¶ ‡≤í‡≤¶‡≥ç‡≤¶‡≤æ‡≤°‡≥ç‡≤§‡≤ø‡≤¶‡≥ç‡≤¶‡≥Ü, ‡≤Ü‡≤¶‡≥ç‡≤∞‡≥Ü ‡≤Ö‡≤¶‡≥ç‡≤®‡≥Ä‡≤ó ‡≤Æ‡≥Ü‡≤ï‡≤æ‡≤®‡≤ø‡≤ï‡≥ç ‡≤Ü‡≤ó‡≤ø‡≤∞‡≥ã ‡≤®‡≤ø‡≤Æ‡≥ç‚Äå ‡≤∏‡≤π‡≤æ‡≤Ø‡≥ç‡≤¶‡≤ø‡≤Ç‡≤¶ ‡≤¨‡≤ó‡≥Ü‡≤π‡≤∞‡≤ø‡≤∏‡≥ç‡≤ï‡≥ã‡≤¨‡≥ã‡≤¶‡≥Å ‡≤Ö‡≤Ç‡≤§‡≤æ‡≤ó‡≤ø ‡≤®‡≤ø‡≤∞‡≤æ‡≤≥ ‡≤Ü‡≤Ø‡≥ç‡≤§‡≥Å ‡≤®‡≤Ç‡≤ó‡≥Ü.",
        "synth_text": "‡¶ö‡ßá‡¶®‡ßç‡¶®‡¶æ‡¶á‡ßü‡ßá‡¶∞ ‡¶∂‡ßá‡ßü‡¶æ‡¶∞‡ßá‡¶∞ ‡¶Ö‡¶ü‡ßã‡¶∞ ‡¶Ø‡¶æ‡¶§‡ßç‡¶∞‡ßÄ‡¶¶‡ßá‡¶∞ ‡¶Æ‡¶ß‡ßç‡¶Ø‡ßá ‡¶ñ‡¶æ‡¶¨‡¶æ‡¶∞ ‡¶≠‡¶æ‡¶ó ‡¶ï‡¶∞‡ßá ‡¶ñ‡¶æ‡¶ì‡ßü‡¶æ‡¶ü‡¶æ ‡¶Ü‡¶Æ‡¶æ‡¶∞ ‡¶ï‡¶æ‡¶õ‡ßá ‡¶Æ‡¶® ‡¶ñ‡ßÅ‡¶¨ ‡¶≠‡¶æ‡¶≤‡ßã ‡¶ï‡¶∞‡ßá ‡¶¶‡ßá‡¶ì‡ßü‡¶æ ‡¶è‡¶ï‡¶ü‡¶æ ‡¶¨‡¶ø‡¶∑‡ßü‡•§"
    },
]

# ------------------------------------------------------------
#  6Ô∏è‚É£  Pre‚Äëload example audio files
# ------------------------------------------------------------
for ex in EXAMPLES:
    sr, data = load_audio_from_url(ex["audio_url"])
    ex["sample_rate"] = sr
    ex["audio_data"] = data

# ------------------------------------------------------------
#  7Ô∏è‚É£  Gradio UI
# ------------------------------------------------------------
with gr.Blocks() as iface:
    gr.Markdown(
        """
        # **IndicF5: High‚ÄëQuality Text‚Äëto‚ÄëSpeech for Indian Languages**

        [![Hugging Face](https://img.shields.io/badge/HuggingFace-Model-orange)](https://huggingface.co/ai4bharat/IndicF5)

        Generate speech using a reference prompt audio and its transcript.
        """
    )
    with gr.Row():
        with gr.Column():
            txt = gr.Textbox(label="Text to Synthesize", placeholder="Enter text...", lines=3)
            ref_audio = gr.Audio(label="Reference Prompt Audio", type="numpy")
            ref_txt = gr.Textbox(label="Reference Text", placeholder="Enter transcript...", lines=2)
            btn = gr.Button("üé§ Generate Speech", variant="primary")
        with gr.Column():
            out = gr.Audio(label="Generated Speech", type="numpy")
    # Examples grid
    examples = [
        [ex["synth_text"], (ex["sample_rate"], ex["audio_data"]), ex["ref_text"]]
        for ex in EXAMPLES
    ]
    gr.Examples(
        examples=examples,
        inputs=[txt, ref_audio, ref_txt],
        label="Choose an example:",
    )
    btn.click(synthesize_speech, inputs=[txt, ref_audio, ref_txt], outputs=[out])

iface.launch()
