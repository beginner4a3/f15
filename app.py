import io
import os
import spaces
try:
    gpu_decorator = spaces.GPU
except AttributeError:
    # If spaces.GPU is not found or import failed partially
    def gpu_decorator(func):
        return func
except ImportError:
    # If spaces is not installed
    def gpu_decorator(func):
        return func
import torch
import librosa
import requests
import tempfile
import numpy as np
import gradio as gr
import soundfile as sf
from transformers import AutoModel

# ------------------------------------------------------------
#  1я╕ПтГг  FlashтАСAttention / SDPA & TF32 settings (run once)
# ------------------------------------------------------------
if torch.cuda.is_available():
    # FlashтАСAttention / SDPA
    try:
        torch.backends.cuda.enable_flash_sdp(True)
        torch.backends.cuda.enable_mem_efficient_sdp(True)
        print("тЬЕ Flash Attention / SDPA enabled")
    except Exception as e:
        print(f"тЪая╕П Flash Attention not available: {e}")

    # TF32 for faster matrix ops on Ampere+ GPUs
    torch.backends.cuda.matmul.allow_tf32 = True
    torch.backends.cudnn.allow_tf32 = True
    torch.backends.cudnn.benchmark = True
    print("тЬЕ CUDA optimizations enabled (TF32, cuDNN benchmark)")

# ------------------------------------------------------------
#  2я╕ПтГг  Helper: download reference audio
# ------------------------------------------------------------
def load_audio_from_url(url: str):
    resp = requests.get(url)
    if resp.status_code == 200:
        audio_data, sr = sf.read(io.BytesIO(resp.content))
        return sr, audio_data
    return None, None

# ------------------------------------------------------------
#  3я╕ПтГг  Model loading тАУ FP16 + compile
# ------------------------------------------------------------
repo_id = "ai4bharat/IndicF5"
print("Loading model:", repo_id)

# Load directly in halfтАСprecision (FP16)
model = AutoModel.from_pretrained(
    repo_id,
    trust_remote_code=True,
    low_cpu_mem_usage=False,          # must stay False тАУ prevents meta tensors
    torch_dtype=torch.float16,         # FP16 for speed
    token=os.getenv("HF_TOKEN") if os.getenv("HF_TOKEN") else None,
)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Device:", device)
model = model.to(device)

# Optional torch.compile тАУ DISABLED: causes 3-5 min delay on first inference
# if hasattr(torch, "compile"):
#     try:
#         model = torch.compile(model, mode="reduce-overhead")
#         print("тЬЕ torch.compile enabled (reduce-overhead)")
#     except Exception as e:
#         print(f"тЪая╕П torch.compile failed: {e}")

# ------------------------------------------------------------
#  4я╕ПтГг  Inference wrapper тАУ inference mode + autocast
# ------------------------------------------------------------
@gpu_decorator
def synthesize_speech(text, ref_audio, ref_text):
    # Basic validation
    if ref_audio is None or not ref_text.strip():
        return "Error: Please provide a reference audio and its corresponding text."

    # Unpack reference audio (Gradio gives (sr, np.ndarray))
    if isinstance(ref_audio, tuple) and len(ref_audio) == 2:
        sample_rate, audio_data = ref_audio
    else:
        return "Error: Invalid reference audio input."

    # Write temporary wav (no resampling needed)
    with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp_wav:
        sf.write(tmp_wav.name, audio_data, samplerate=sample_rate, format="WAV")
        tmp_wav.flush()

    # Fast inference
    with torch.inference_mode(), torch.amp.autocast('cuda', enabled=True):
        # The IndicF5 model implements __call__ as the inference entry point
        audio = model(text, ref_audio_path=tmp_wav.name, ref_text=ref_text)

    # Debug: print audio info
    print(f"[DEBUG] Audio type: {type(audio)}, dtype: {audio.dtype if hasattr(audio, 'dtype') else 'N/A'}")
    print(f"[DEBUG] Audio shape: {audio.shape if hasattr(audio, 'shape') else len(audio)}")
    print(f"[DEBUG] Audio min: {audio.min()}, max: {audio.max()}")

    # Convert to numpy if tensor
    if hasattr(audio, 'cpu'):
        audio = audio.cpu().numpy()

    # Ensure 1D audio
    if len(audio.shape) > 1:
        audio = audio.squeeze()

    # Normalize to [-1, 1] range for proper playback
    if audio.dtype == np.int16:
        audio = audio.astype(np.float32) / 32768.0
    elif audio.dtype == np.float32 or audio.dtype == np.float64:
        # Normalize if not already in [-1, 1]
        max_val = np.abs(audio).max()
        if max_val > 1.0:
            audio = audio / max_val
    else:
        audio = audio.astype(np.float32)

    print(f"[DEBUG] Final audio shape: {audio.shape}, range: [{audio.min():.4f}, {audio.max():.4f}]")

    return 24000, audio

# ------------------------------------------------------------
#  5я╕ПтГг  Example data (unchanged, just kept for UI)
# ------------------------------------------------------------
EXAMPLES = [
    {
        "audio_name": "PAN_F (Happy)",
        "audio_url": "https://github.com/AI4Bharat/IndicF5/raw/refs/heads/main/prompts/PAN_F_HAPPY_00002.wav",
        "ref_text": "риЗрй▒риХ риЧрйНри░ри╛ри╣риХ риирйЗ ри╕ри╛рибрйА римрйЗриори┐ри╕ри╛ри▓ ри╕рйЗри╡ри╛ римри╛ри░рйЗ рижри┐ри▓рйЛриВриЧри╡ри╛ри╣рйА рижри┐рй▒ридрйА риЬри┐ри╕ риири╛ри▓ ри╕ри╛риирйВрй░ риЕриирй░риж риори╣ри┐ри╕рйВри╕ ри╣рйЛриЗриЖред",
        "synth_text": "рдореИрдВ рдмрд┐рдирд╛ рдХрд┐рд╕реА рдЪрд┐рдВрддрд╛ рдХреЗ рдЕрдкрдиреЗ рджреЛрд╕реНрддреЛрдВ рдХреЛ рдЕрдкрдиреЗ рдСрдЯреЛрдореЛрдмрд╛рдЗрд▓ рдПрдХреНрд╕рдкрд░реНрдЯ рдХреЗ рдкрд╛рд╕ рднреЗрдЬ рджреЗрддрд╛ рд╣реВрдБ рдХреНрдпреЛрдВрдХрд┐ рдореИрдВ рдЬрд╛рдирддрд╛ рд╣реВрдБ рдХрд┐ рд╡рд╣ рдирд┐рд╢реНрдЪрд┐рдд рд░реВрдк рд╕реЗ рдЙрдирдХреА рд╕рднреА рдЬрд░реВрд░рддреЛрдВ рдкрд░ рдЦрд░рд╛ рдЙрддрд░реЗрдЧрд╛ред"
    },
    {
        "audio_name": "TAM_F (Happy)",
        "audio_url": "https://github.com/AI4Bharat/IndicF5/raw/refs/heads/main/prompts/TAM_F_HAPPY_00001.wav",
        "ref_text": "роиро╛ройрпН роирпЖройроЪрпНроЪ рооро╛родро┐ро░ро┐ропрпЗ роЕроорпЗроЪро╛ройрпНро▓ рокрпЖро░ро┐роп родро│рпНро│рпБрокроЯро┐ ро╡роирпНродро┐ро░рпБроХрпНроХрпБ. роХроорпНрооро┐ роХро╛роЪрпБроХрпНроХрпЗ роЕроирпНродрокрпН рокрпБродрпБ роЪрпЗроорпНроЪроЩрпН рооро╛роЯро▓ ро╡ро╛роЩрпНроХро┐роЯро▓ро╛роорпН.",
        "synth_text": "р┤нр┤Хр╡Нр┤╖р┤гр┤др╡Нр┤др┤┐р┤ир╡Н р┤╢р╡Зр┤╖р┤В р┤др╡Ир┤░р╡Н р┤╕р┤╛р┤жр┤В р┤Хр┤┤р┤┐р┤Ър╡Нр┤Ър┤╛р╡╜ р┤Тр┤░р╡Б р┤Йр┤╖р┤╛р┤▒р┤╛р┤гр╡Н!"
    },
    {
        "audio_name": "MAR_F (WIKI)",
        "audio_url": "https://github.com/AI4Bharat/IndicF5/raw/refs/heads/main/prompts/MAR_F_WIKI_00001.wav",
        "ref_text": "рджрд┐рдЧрдВрдЯрд░рд╛рд╡реНрджрд╛рд░реЗ рдЕрдВрддрд░рд╛рд│ рдХрдХреНрд╖реЗрддрд▓рд╛ рдХрдЪрд░рд╛ рдЪрд┐рдиреНрд╣рд┐рдд рдХрд░рдгреНрдпрд╛рд╕рд╛рдареА рдкреНрд░рдпрддреНрди рдХреЗрд▓реЗ рдЬрд╛рдд рдЖрд╣реЗ.",
        "synth_text": "рдкреНрд░рд╛рд░рдВрднрд┐рдХ рдЕрдВрдХреБрд░ рдЫреЗрджрдХ. рдореА рд╕реЛрд▓рд╛рдкреВрд░ рдЬрд┐рд▓реНрд╣реНрдпрд╛рддреАрд▓ рдорд╛рд│рд╢рд┐рд░рд╕ рддрд╛рд▓реБрдХреНрдпрд╛рддреАрд▓ рд╢реЗрддрдХрд░реА рдЧрдгрдкрдд рдкрд╛рдЯреАрд▓ рдмреЛрд▓рддреЛрдп. рдорд╛рдЭреНрдпрд╛ рдКрд╕ рдкрд┐рдХрд╛рд╡рд░ рдкреНрд░рд╛рд░рдВрднрд┐рдХ рдЕрдВрдХреБрд░ рдЫреЗрджрдХ рдХреАрдб рдЖрдврд│рдд рдЖрд╣реЗ. рдХреНрд▓реЛрд░рдБрдЯреНрд░рд╛рдирд┐рд▓реАрдкреНрд░реЛрд▓ (рдХреЛрд░рд╛рдЬреЗрди) рд╡рд╛рдкрд░рдгреЗ рдпреЛрдЧреНрдп рдЖрд╣реЗ рдХрд╛? рддреНрдпрд╛рдЪреЗ рдкреНрд░рдорд╛рдг рдХрд┐рддреА рдЕрд╕рд╛рд╡реЗ?"
    },
    {
        "audio_name": "MAR_M (WIKI)",
        "audio_url": "https://github.com/AI4Bharat/IndicF5/raw/refs/heads/main/prompts/MAR_M_WIKI_00001.wav",
        "ref_text": "рдпрд╛ рдкреНрд░рдерд╛рд▓рд╛ рдПрдХреЛрдгреАрд╕рд╢реЗ рдкрдВрдЪрд╛рддрд░ рдИрд╕рд╡реА рдкрд╛рд╕реВрди рднрд╛рд░рддреАрдп рджрдВрдб рд╕рдВрд╣рд┐рддрд╛рдЪреА рдзрд╛рд░рд╛ рдЪрд╛рд░рд╢реЗ рдЕрдареНрдард╛рд╡реАрд╕ рдЖрдгрд┐ рдЪрд╛рд░рд╢реЗ рдПрдХреЛрдгрддреАрд╕рдЪреНрдпрд╛ рдЕрдиреНрддрд░реНрдЧрдд рдирд┐рд╖реЗрдз рдХреЗрд▓рд╛.",
        "synth_text": "рдЬреАрд╡рд╛рдгреВ рдХрд░рдкрд╛. рдореА рдЕрд╣рдорджрдирдЧрд░ рдЬрд┐рд▓реНрд╣реНрдпрд╛рддреАрд▓ рд░рд╛рд╣реБрд░реА рдЧрд╛рд╡рд╛рддреВрди рдмрд╛рд│рд╛рд╕рд╛рд╣реЗрдм рдЬрд╛рдзрд╡ рдмреЛрд▓рддреЛрдп. рдорд╛рдЭреНрдпрд╛ рдбрд╛рд│рд┐рдВрдм рдмрд╛рдЧреЗрдд рдЬреАрд╡рд╛рдгреВ рдХрд░рдкрд╛ рдореЛрдареНрдпрд╛ рдкреНрд░рдорд╛рдгрд╛рдд рджрд┐рд╕рддреЛрдп. рд╕реНрдЯреНрд░реЗрдкреНрдЯреЛрд╕рд╛рдпрдХреНрд▓рд┐рди рдЖрдгрд┐ рдХреЙрдкрд░ рдСрдХреНрд╕рд┐рдбреНрд▓реЛрд░рд╛рдИрдб рдлрд╡рд╛рд░рдгреАрд╕рд╛рдареА рдпреЛрдЧреНрдп рдкреНрд░рдорд╛рдг рдХрд╛рдп рдЕрд╕рд╛рд╡реЗ?"
    },
    {
        "audio_name": "KAN_F (Happy)",
        "audio_url": "https://github.com/AI4Bharat/IndicF5/raw/refs/heads/main/prompts/KAN_F_HAPPY_00001.wav",
        "ref_text": "р▓ир▓ор│НтАМ р▓лр│Нр▓░р▓┐р▓Ьр│Нр▓Ьр▓▓р│Нр▓▓р▓┐  р▓Хр│Вр▓▓р▓┐р▓Вр▓Чр│НтАМ р▓╕р▓ор▓╕р│Нр▓пр│Ж р▓Жр▓Чр▓┐ р▓ир▓╛р▓ир│НтАМ р▓нр▓╛р▓│ р▓жр▓┐р▓ир▓жр▓┐р▓Вр▓ж р▓Тр▓жр│Нр▓жр▓╛р▓бр│Нр▓др▓┐р▓жр│Нр▓жр│Ж, р▓Жр▓жр│Нр▓░р│Ж р▓Ер▓жр│Нр▓ир│Ар▓Ч р▓ор│Жр▓Хр▓╛р▓ир▓┐р▓Хр│Н р▓Жр▓Чр▓┐р▓░р│Л р▓ир▓┐р▓ор│НтАМ р▓╕р▓╣р▓╛р▓пр│Нр▓жр▓┐р▓Вр▓ж р▓мр▓Чр│Жр▓╣р▓░р▓┐р▓╕р│Нр▓Хр│Лр▓мр│Лр▓жр│Б р▓Ер▓Вр▓др▓╛р▓Чр▓┐ р▓ир▓┐р▓░р▓╛р▓│ р▓Жр▓пр│Нр▓др│Б р▓ир▓Вр▓Чр│Ж.",
        "synth_text": "ржЪрзЗржирзНржирж╛ржЗрзЯрзЗрж░ рж╢рзЗрзЯрж╛рж░рзЗрж░ ржЕржЯрзЛрж░ ржпрж╛рждрзНрж░рзАржжрзЗрж░ ржоржзрзНржпрзЗ ржЦрж╛ржмрж╛рж░ ржнрж╛ржЧ ржХрж░рзЗ ржЦрж╛ржУрзЯрж╛ржЯрж╛ ржЖржорж╛рж░ ржХрж╛ржЫрзЗ ржоржи ржЦрзБржм ржнрж╛рж▓рзЛ ржХрж░рзЗ ржжрзЗржУрзЯрж╛ ржПржХржЯрж╛ ржмрж┐рж╖рзЯред"
    },
]

# ------------------------------------------------------------
#  6я╕ПтГг  PreтАСload example audio files
# ------------------------------------------------------------
for ex in EXAMPLES:
    sr, data = load_audio_from_url(ex["audio_url"])
    ex["sample_rate"] = sr
    ex["audio_data"] = data

# ------------------------------------------------------------
#  7я╕ПтГг  Gradio UI
# ------------------------------------------------------------
with gr.Blocks() as iface:
    gr.Markdown(
        """
        # **IndicF5: HighтАСQuality TextтАСtoтАСSpeech for Indian Languages**

        [![Hugging Face](https://img.shields.io/badge/HuggingFace-Model-orange)](https://huggingface.co/ai4bharat/IndicF5)

        Generate speech using a reference prompt audio and its transcript.
        """
    )
    with gr.Row():
        with gr.Column():
            txt = gr.Textbox(label="Text to Synthesize", placeholder="Enter text...", lines=3)
            ref_audio = gr.Audio(label="Reference Prompt Audio", type="numpy")
            ref_txt = gr.Textbox(label="Reference Text", placeholder="Enter transcript...", lines=2)
            btn = gr.Button("ЁЯОд Generate Speech", variant="primary")
        with gr.Column():
            out = gr.Audio(label="Generated Speech", type="numpy")
    # Examples grid
    examples = [
        [ex["synth_text"], (ex["sample_rate"], ex["audio_data"]), ex["ref_text"]]
        for ex in EXAMPLES
    ]
    gr.Examples(
        examples=examples,
        inputs=[txt, ref_audio, ref_txt],
        label="Choose an example:",
    )
    btn.click(synthesize_speech, inputs=[txt, ref_audio, ref_txt], outputs=[out])

if __name__ == "__main__":
    iface.launch(share=True, debug=True)
